{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Chatbots with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /usr/lib/python3/dist-packages (0.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chatbot (no RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI(\n",
    "   openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "   model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string relativite by eninstein.\")\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe you may be referring to the concept of \"space-time\" and the theory of relativity proposed by Albert Einstein. In his theory, Einstein combined the concepts of space and time into a single unified entity known as space-time. This theory, called the theory of special relativity, introduced the idea that the laws of physics are the same for all observers moving at constant velocities relative to each other.\n",
      "\n",
      "However, I'm not sure what you mean by \"string relativite.\" Could you please clarify your question?\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for any confusion caused by my previous response. To clarify, in Euclidean geometry, the shortest distance between two points is indeed a straight line. In a flat, two-dimensional Euclidean space, such as a piece of paper, a straight line connecting two points is the shortest distance.\n",
      "\n",
      "However, in non-Euclidean geometries, such as spherical or hyperbolic geometries, the shortest distance between two points can be a curve. In these curved spaces, the geometry itself is different from that of flat Euclidean space, leading to different behaviors and properties.\n",
      "\n",
      "For example, on the surface of a sphere, like the Earth, the shortest path between two points is a great circle, which is a curve. This is because the surface of a sphere is positively curved, and straight lines (geodesics) on a sphere are curves.\n",
      "\n",
      "In the context of Einstein's theory of general relativity, the curvature of space-time is related to the presence of mass and energy. Massive objects, such as planets or stars, cause a curvature in space-time, and the path of an object moving through this curved space-time is influenced by this curvature. In this theory, the shortest path between two points in space-time is determined by the curvature caused by the distribution of mass and energy.\n",
      "\n",
      "So, to summarize, the shortest distance between points in a space depends on the geometry of that space. In flat Euclidean space, it is a straight line, while in curved spaces or in the presence of gravitational fields, it can be a curved path.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why is the shortes distance between points in a space is a curved line'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about the new (and very popular) Llama 2 LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I couldn't find any specific information about \"Llama 2.\" It's possible that you may be referring to something specific that I'm not aware of. Could you please provide more context or clarify your question?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect.\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I couldn't find any specific information about \"LLMChain\" in relation to \"LangChain.\" It's possible that these terms are specific to a certain context or domain that I'm not familiar with. Without more information, I'm unable to provide you with accurate details.\n",
      "\n",
      "If you can provide additional context or clarify your question, I'll do my best to assist you further.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMChain refers to a specific type of chain within the LangChain framework for developing applications powered by language models. In the context provided, an LLMChain is described as the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n",
      "\n",
      "The purpose of an LLMChain is to take multiple input variables, format them using the PromptTemplate, and pass the resulting prompt to the language model (either an LLM or a ChatModel). The model generates an output, and if an OutputParser is provided, it is used to parse and format the output into a final desired format.\n",
      "\n",
      "The LangChain framework aims to enable powerful and differentiated applications by not only calling out to a language model via an API but also by being data-aware and allowing the language model to interact with its environment. LLMChain is an integral part of this framework, providing a structured approach to utilizing language models and connecting them with other data sources.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/llama-2-arxiv-papers\"` dataset. This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/abrham/.local/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: xxhash in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: multiprocess in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: packaging in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: filelock in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: pandas in /home/abrham/.local/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/abrham/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abrham/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abrham/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/abrham/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/abrham/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Academy\n",
      "Cohort\n",
      "A\n",
      "Weekly\n",
      "Challenge:\n",
      "Week\n",
      "6\n",
      "Precision\n",
      "RAG:\n",
      "Prompt\n",
      "Tuning\n",
      "For\n",
      "Building\n",
      "Enterprise\n",
      "Grade\n",
      "RAG\n",
      "Systems\n",
      "Business\n",
      "objective\n",
      "PromptlyTech\n",
      "is\n",
      "an\n",
      "innovative\n",
      "e-business\n",
      "specializing\n",
      "in\n",
      "providing\n",
      "AI-driven\n",
      "solutions\n",
      "for\n",
      "optimizing\n",
      "the\n",
      "use\n",
      "of\n",
      "Language\n",
      "Models\n",
      "(LLMs)\n",
      "in\n",
      "various\n",
      "industries.\n",
      "The\n",
      "company\n",
      "aims\n",
      "to\n",
      "revolutionize\n",
      "how\n",
      "businesses\n",
      "interact\n",
      "with\n",
      "LLMs,\n",
      "making\n",
      "the\n",
      "technology\n",
      "more\n",
      "accessible,\n",
      "eﬃcient,\n",
      "and\n",
      "eﬀective.\n",
      "By\n",
      "addressing\n",
      "the\n",
      "challenges\n",
      "of\n",
      "prompt\n",
      "engineering,\n",
      "the\n",
      "company\n",
      "plays\n",
      "a\n",
      "pivotal\n",
      "role\n",
      "in\n",
      "enhancing\n",
      "decision-making,\n",
      "operational\n",
      "eﬃciency,\n",
      "and\n",
      "customer\n",
      "experience\n",
      "across\n",
      "various\n",
      "industries.\n",
      "PromptlyTech's\n",
      "solutions\n",
      "are\n",
      "designed\n",
      "to\n",
      "cater\n",
      "to\n",
      "the\n",
      "evolving\n",
      "needs\n",
      "of\n",
      "a\n",
      "digitally-driven\n",
      "business\n",
      "landscape,\n",
      "where\n",
      "speed\n",
      "and\n",
      "accuracy\n",
      "are\n",
      "key\n",
      "to\n",
      "staying\n",
      "competitive.\n",
      "The\n",
      "company\n",
      "focuses\n",
      "on\n",
      "key\n",
      "services:\n",
      "Automatic\n",
      "Prompt\n",
      "Generation,\n",
      "Automatic\n",
      "Evaluation\n",
      "Data\n",
      "Generation,\n",
      "and\n",
      "Prompt\n",
      "Testing\n",
      "and\n",
      "Ranking.\n",
      "1.\n",
      "Automatic\n",
      "Prompt\n",
      "Generation\n",
      "Service:\n",
      "●\n",
      "This\n",
      "service\n",
      "streamlines\n",
      "the\n",
      "process\n",
      "of\n",
      "creating\n",
      "eﬀective\n",
      "prompts,\n",
      "enabling\n",
      "businesses\n",
      "to\n",
      "eﬃciently\n",
      "utilize\n",
      "LLMs\n",
      "for\n",
      "generating\n",
      "high-quality,\n",
      "relevant\n",
      "content.\n",
      "It\n",
      "signiﬁcantly\n",
      "reduces\n",
      "the\n",
      "time\n",
      "and\n",
      "expertise\n",
      "required\n",
      "in\n",
      "crafting\n",
      "prompts\n",
      "manually.\n",
      "2.\n",
      "Automatic\n",
      "Evaluation\n",
      "Data\n",
      "Generation\n",
      "Service:\n",
      "●\n",
      "PromptlyTech’s\n",
      "service\n",
      "automates\n",
      "the\n",
      "generation\n",
      "of\n",
      "diverse\n",
      "test\n",
      "cases,\n",
      "ensuring\n",
      "comprehensive\n",
      "coverage\n",
      "and\n",
      "identifying\n",
      "potential\n",
      "issues.\n",
      "This\n",
      "enhances\n",
      "the\n",
      "reliability\n",
      "and\n",
      "performance\n",
      "of\n",
      "LLM\n",
      "applications,\n",
      "saving\n",
      "signiﬁcant\n",
      "time\n",
      "in\n",
      "the\n",
      "QA(Quality\n",
      "Assurance)\n",
      "process.\n",
      "3.\n",
      "Prompt\n",
      "Testing\n",
      "and\n",
      "Ranking\n",
      "Service:\n",
      "●\n",
      "PromptlyTech’s\n",
      "service\n",
      "evaluates\n",
      "and\n",
      "ranks\n",
      "diﬀerent\n",
      "prompts\n",
      "based\n",
      "on\n",
      "eﬀectiveness,\n",
      "helping\n",
      "Users\n",
      "to\n",
      "get\n",
      "the\n",
      "desired\n",
      "outcome\n",
      "from\n",
      "LLM.\n",
      "It\n",
      "ensures\n",
      "that\n",
      "chatbots\n",
      "and\n",
      "virtual\n",
      "assistants\n",
      "provide\n",
      "accurate,\n",
      "contextually\n",
      "relevant\n",
      "responses,\n",
      "thereby\n",
      "improving\n",
      "user\n",
      "engagement\n",
      "and\n",
      "satisfaction.\n",
      "Background\n",
      "Context\n",
      "In\n",
      "the\n",
      "evolving\n",
      "ﬁeld\n",
      "of\n",
      "artiﬁcial\n",
      "intelligence,\n",
      "Language\n",
      "Models\n",
      "(LLMs)\n",
      "like\n",
      "GPT-3.5\n",
      "and\n",
      "GPT-4\n",
      "have\n",
      "become\n",
      "crucial\n",
      "for\n",
      "various\n",
      "applications.\n",
      "Their\n",
      "eﬀectiveness,\n",
      "however,\n",
      "heavily\n",
      "depends\n",
      "on\n",
      "the\n",
      "quality\n",
      "of\n",
      "the\n",
      "prompts\n",
      "they\n",
      "receive,\n",
      "leading\n",
      "to\n",
      "the\n",
      "emergence\n",
      "of\n",
      "\"prompt\n",
      "engineering\"\n",
      "as\n",
      "a\n",
      "key\n",
      "skill.\n",
      "Prompt\n",
      "engineering\n",
      "is\n",
      "the\n",
      "craft\n",
      "of\n",
      "designing\n",
      "queries\n",
      "or\n",
      "statements\n",
      "to\n",
      "guide\n",
      "LLMs\n",
      "to\n",
      "produce\n",
      "desired\n",
      "outcomes.\n",
      "The\n",
      "challenge\n",
      "lies\n",
      "in\n",
      "the\n",
      "sensitivity\n",
      "of\n",
      "these\n",
      "models\n",
      "to\n",
      "prompt\n",
      "nuances,\n",
      "where\n",
      "slight\n",
      "variations\n",
      "can\n",
      "yield\n",
      "vastly\n",
      "diﬀerent\n",
      "results.\n",
      "This\n",
      "poses\n",
      "a\n",
      "signiﬁcant\n",
      "hurdle\n",
      "for\n",
      "users,\n",
      "especially\n",
      "in\n",
      "business\n",
      "contexts\n",
      "where\n",
      "accuracy\n",
      "and\n",
      "relevance\n",
      "are\n",
      "paramount.\n",
      "The\n",
      "need\n",
      "for\n",
      "simpliﬁed,\n",
      "eﬃcient\n",
      "prompt\n",
      "engineering\n",
      "is\n",
      "clear.\n",
      "Automating\n",
      "and\n",
      "optimizing\n",
      "this\n",
      "process\n",
      "can\n",
      "save\n",
      "time,\n",
      "enhance\n",
      "LLM\n",
      "productivity,\n",
      "and\n",
      "make\n",
      "advanced\n",
      "AI\n",
      "capabilities\n",
      "more\n",
      "accessible\n",
      "to\n",
      "a\n",
      "broader\n",
      "range\n",
      "of\n",
      "users.\n",
      "The\n",
      "tasks\n",
      "of\n",
      "Automatic\n",
      "Prompt\n",
      "Generation,\n",
      "Evaluation\n",
      "Data\n",
      "Generation,\n",
      "and\n",
      "Prompt\n",
      "Testing\n",
      "and\n",
      "Ranking\n",
      "are\n",
      "aimed\n",
      "at\n",
      "addressing\n",
      "these\n",
      "challenges,\n",
      "streamlining\n",
      "the\n",
      "prompt\n",
      "engineering\n",
      "process\n",
      "for\n",
      "more\n",
      "eﬀective\n",
      "use\n",
      "of\n",
      "LLMs.\n",
      "Learning\n",
      "Outcomes\n",
      "Skills\n",
      "Development\n",
      "●\n",
      "Prompt\n",
      "Engineering\n",
      "Proﬁciency:\n",
      "Gain\n",
      "expertise\n",
      "in\n",
      "crafting\n",
      "eﬀective\n",
      "prompts\n",
      "that\n",
      "guide\n",
      "LLMs\n",
      "to\n",
      "desired\n",
      "outputs,\n",
      "understanding\n",
      "nuances\n",
      "and\n",
      "variations\n",
      "in\n",
      "language\n",
      "that\n",
      "impact\n",
      "model\n",
      "responses.\n",
      "●\n",
      "Critical\n",
      "Analysis:\n",
      "Develop\n",
      "the\n",
      "ability\n",
      "to\n",
      "critically\n",
      "analyze\n",
      "and\n",
      "evaluate\n",
      "the\n",
      "eﬀectiveness\n",
      "of\n",
      "diﬀerent\n",
      "prompts\n",
      "based\n",
      "on\n",
      "their\n",
      "performance\n",
      "in\n",
      "varied\n",
      "scenarios.\n",
      "●\n",
      "Technical\n",
      "Aptitude\n",
      "with\n",
      "LLMs:\n",
      "Enhance\n",
      "technical\n",
      "skills\n",
      "in\n",
      "using\n",
      "advanced\n",
      "language\n",
      "models\n",
      "like\n",
      "GPT-4\n",
      "and\n",
      "GPT-3.5-Turbo,\n",
      "understanding\n",
      "their\n",
      "functionalities\n",
      "and\n",
      "capabilities.\n",
      "●\n",
      "Problem-Solving\n",
      "and\n",
      "Creativity:\n",
      "Cultivate\n",
      "creative\n",
      "problem-solving\n",
      "skills\n",
      "by\n",
      "generating\n",
      "innovative\n",
      "prompts\n",
      "and\n",
      "test\n",
      "cases,\n",
      "addressing\n",
      "complex\n",
      "and\n",
      "varied\n",
      "objectives.●\n",
      "Data\n",
      "Interpretation:\n",
      "Learn\n",
      "to\n",
      "interpret\n",
      "and\n",
      "analyze\n",
      "data\n",
      "from\n",
      "test\n",
      "cases\n",
      "and\n",
      "prompt\n",
      "evaluations,\n",
      "deriving\n",
      "meaningful\n",
      "insights\n",
      "from\n",
      "performance\n",
      "metrics.\n",
      "Knowledge\n",
      "Acquisition\n",
      "●\n",
      "Understanding\n",
      "of\n",
      "Language\n",
      "Models:\n",
      "Acquire\n",
      "a\n",
      "deeper\n",
      "understanding\n",
      "of\n",
      "how\n",
      "LLMs\n",
      "function,\n",
      "including\n",
      "their\n",
      "strengths,\n",
      "limitations,\n",
      "and\n",
      "the\n",
      "principles\n",
      "behind\n",
      "their\n",
      "responses.\n",
      "●\n",
      "Insights\n",
      "into\n",
      "Automated\n",
      "Evaluation\n",
      "Data\n",
      "Generation:\n",
      "Gain\n",
      "knowledge\n",
      "about\n",
      "the\n",
      "methodology\n",
      "and\n",
      "importance\n",
      "of\n",
      "creating\n",
      "test\n",
      "cases\n",
      "for\n",
      "evaluating\n",
      "prompt\n",
      "eﬀectiveness.\n",
      "●\n",
      "ELO\n",
      "Rating\n",
      "System\n",
      "and\n",
      "its\n",
      "Applications:\n",
      "Learn\n",
      "about\n",
      "the\n",
      "ELO\n",
      "rating\n",
      "system\n",
      "used\n",
      "for\n",
      "ranking\n",
      "prompts,\n",
      "understanding\n",
      "its\n",
      "mechanics\n",
      "and\n",
      "relevance\n",
      "in\n",
      "performance\n",
      "evaluation.\n",
      "●\n",
      "Prompt\n",
      "Optimization\n",
      "Strategies:\n",
      "Understand\n",
      "various\n",
      "strategies\n",
      "for\n",
      "reﬁning\n",
      "and\n",
      "optimizing\n",
      "prompts\n",
      "to\n",
      "achieve\n",
      "better\n",
      "alignment\n",
      "with\n",
      "speciﬁc\n",
      "goals\n",
      "and\n",
      "desired\n",
      "outcomes.\n",
      "●\n",
      "Industry\n",
      "Best\n",
      "Practices:\n",
      "Familiarize\n",
      "with\n",
      "the\n",
      "best\n",
      "practices\n",
      "in\n",
      "prompt\n",
      "engineering\n",
      "within\n",
      "diﬀerent\n",
      "industries,\n",
      "learning\n",
      "about\n",
      "real-world\n",
      "applications\n",
      "and\n",
      "challenges.\n",
      "Team\n",
      "Tutors:\n",
      "-\n",
      "Yabebal\n",
      "-\n",
      "Emitinan\n",
      "-\n",
      "Rehmet\n",
      "Badges\n",
      "Each\n",
      "week,\n",
      "one\n",
      "user\n",
      "will\n",
      "be\n",
      "awarded\n",
      "one\n",
      "of\n",
      "the\n",
      "badges\n",
      "below\n",
      "for\n",
      "the\n",
      "best\n",
      "performance\n",
      "in\n",
      "the\n",
      "category\n",
      "below.\n",
      "In\n",
      "addition\n",
      "to\n",
      "being\n",
      "the\n",
      "badge\n",
      "holder\n",
      "for\n",
      "that\n",
      "badge,\n",
      "each\n",
      "badge\n",
      "winner\n",
      "will\n",
      "get\n",
      "+20\n",
      "points\n",
      "to\n",
      "the\n",
      "overall\n",
      "score.\n",
      "Visualization\n",
      "-\n",
      "quality\n",
      "of\n",
      "visualizations,\n",
      "understandability,\n",
      "skimmability,\n",
      "choice\n",
      "of\n",
      "visualization\n",
      "Quality\n",
      "of\n",
      "code\n",
      "-\n",
      "reliability,\n",
      "maintainability,\n",
      "eﬃciency,\n",
      "commenting\n",
      "-\n",
      "in\n",
      "future\n",
      "this\n",
      "will\n",
      "be\n",
      "CICD\n",
      "/CML\n",
      "Innovative\n",
      "approach\n",
      "to\n",
      "analysis\n",
      "-using\n",
      "latest\n",
      "algorithms,\n",
      "adding\n",
      "in\n",
      "research\n",
      "paper\n",
      "content\n",
      "and\n",
      "other\n",
      "innovative\n",
      "approaches\n",
      "Writing\n",
      "and\n",
      "presentation\n",
      "-\n",
      "clarity\n",
      "of\n",
      "written\n",
      "outputs,\n",
      "clarity\n",
      "of\n",
      "slides,\n",
      "overall\n",
      "production\n",
      "value\n",
      "Most\n",
      "supportive\n",
      "in\n",
      "the\n",
      "community\n",
      "-\n",
      "helping\n",
      "others,\n",
      "adding\n",
      "links,\n",
      "tutoring\n",
      "those\n",
      "strugglingThe\n",
      "goal\n",
      "of\n",
      "this\n",
      "approach\n",
      "is\n",
      "to\n",
      "support\n",
      "and\n",
      "reward\n",
      "expertise\n",
      "in\n",
      "diﬀerent\n",
      "parts\n",
      "of\n",
      "the\n",
      "Machine\n",
      "learning\n",
      "engineering\n",
      "toolbox.\n",
      "Group\n",
      "Work\n",
      "Policy\n",
      "Everyone\n",
      "has\n",
      "to\n",
      "submit\n",
      "all\n",
      "their\n",
      "work\n",
      "individually.Instruction:\n",
      "Automatic\n",
      "Prompt\n",
      "Engineering\n",
      "Fundamental\n",
      "Tasks\n",
      "The\n",
      "core\n",
      "tasks\n",
      "for\n",
      "this\n",
      "week’s\n",
      "challenge\n",
      "in\n",
      "Automatic\n",
      "Prompt\n",
      "Engineering\n",
      "are\n",
      "outlined\n",
      "below:\n",
      "1.\n",
      "Understand\n",
      "Prompt\n",
      "Engineering\n",
      "Tools\n",
      "and\n",
      "Concepts:\n",
      "Gain\n",
      "a\n",
      "thorough\n",
      "understanding\n",
      "of\n",
      "the\n",
      "tools\n",
      "and\n",
      "theoretical\n",
      "concepts\n",
      "involved\n",
      "in\n",
      "prompt\n",
      "engineering\n",
      "for\n",
      "Language\n",
      "Models\n",
      "(LLMs).\n",
      "2.\n",
      "Familiarize\n",
      "with\n",
      "Language\n",
      "Models:\n",
      "Learn\n",
      "about\n",
      "the\n",
      "capabilities\n",
      "and\n",
      "functionalities\n",
      "of\n",
      "advanced\n",
      "LLMs\n",
      "like\n",
      "GPT-4\n",
      "and\n",
      "GPT-3.5-Turbo.\n",
      "3.\n",
      "Develop\n",
      "a\n",
      "Plan\n",
      "for\n",
      "Prompt\n",
      "Generation\n",
      "and\n",
      "Testing:\n",
      "Create\n",
      "a\n",
      "comprehensive\n",
      "plan\n",
      "that\n",
      "outlines\n",
      "the\n",
      "approach\n",
      "for\n",
      "automated\n",
      "prompt\n",
      "generation,\n",
      "test\n",
      "case\n",
      "creation,\n",
      "and\n",
      "prompt\n",
      "evaluation.\n",
      "4.\n",
      "Set\n",
      "Up\n",
      "a\n",
      "Development\n",
      "Environment:\n",
      "Prepare\n",
      "a\n",
      "suitable\n",
      "development\n",
      "environment\n",
      "that\n",
      "supports\n",
      "the\n",
      "integration\n",
      "and\n",
      "testing\n",
      "of\n",
      "LLMs\n",
      "in\n",
      "the\n",
      "prompt\n",
      "engineering\n",
      "process.\n",
      "5.\n",
      "Design\n",
      "User\n",
      "Interface\n",
      "for\n",
      "Prompt\n",
      "System:\n",
      "Plan\n",
      "and\n",
      "initiate\n",
      "the\n",
      "development\n",
      "of\n",
      "a\n",
      "user-friendly\n",
      "interface\n",
      "for\n",
      "prompt\n",
      "input,\n",
      "reﬁnement,\n",
      "and\n",
      "performance\n",
      "analysis.\n",
      "6.\n",
      "Plan\n",
      "Integration\n",
      "of\n",
      "LLMs:\n",
      "Strategize\n",
      "the\n",
      "integration\n",
      "of\n",
      "LLMs\n",
      "into\n",
      "the\n",
      "prompt\n",
      "system\n",
      "for\n",
      "automated\n",
      "generation\n",
      "and\n",
      "testing.\n",
      "7.\n",
      "Build\n",
      "and\n",
      "Reﬁne\n",
      "Prompt\n",
      "Generation\n",
      "System:\n",
      "Develop\n",
      "the\n",
      "automated\n",
      "prompt\n",
      "generation\n",
      "system,\n",
      "ensuring\n",
      "it\n",
      "aligns\n",
      "with\n",
      "user\n",
      "inputs\n",
      "and\n",
      "objectives.\n",
      "8.\n",
      "Develop\n",
      "Automatic\n",
      "Evaluation\n",
      "Data\n",
      "Generation\n",
      "System:\n",
      "Create\n",
      "a\n",
      "system\n",
      "for\n",
      "generating\n",
      "test\n",
      "cases\n",
      "that\n",
      "evaluate\n",
      "the\n",
      "eﬀectiveness\n",
      "of\n",
      "prompts\n",
      "in\n",
      "various\n",
      "scenarios.\n",
      "9.\n",
      "Implement\n",
      "Prompt\n",
      "Testing\n",
      "and\n",
      "Evaluation\n",
      "Mechanism:\n",
      "Set\n",
      "up\n",
      "testing\n",
      "procedures\n",
      "using\n",
      "Monte\n",
      "Carlo\n",
      "matchmaking\n",
      "and\n",
      "ELO\n",
      "rating\n",
      "systems\n",
      "to\n",
      "evaluate\n",
      "and\n",
      "rank\n",
      "prompts.\n",
      "10.\n",
      "Reﬁne\n",
      "and\n",
      "Optimize\n",
      "System\n",
      "Based\n",
      "on\n",
      "Feedback:\n",
      "Continuously\n",
      "reﬁne\n",
      "the\n",
      "prompt\n",
      "generation\n",
      "and\n",
      "evaluation\n",
      "system\n",
      "based\n",
      "on\n",
      "user\n",
      "feedback\n",
      "and\n",
      "performance\n",
      "data.Task\n",
      "1:\n",
      "Review\n",
      "the\n",
      "Evolution\n",
      "of\n",
      "Automatic\n",
      "Prompt\n",
      "Engineering\n",
      "Focus\n",
      "on\n",
      "understanding\n",
      "the\n",
      "key\n",
      "developments\n",
      "in\n",
      "the\n",
      "ﬁeld\n",
      "of\n",
      "automatic\n",
      "prompt\n",
      "engineering\n",
      "for\n",
      "Language\n",
      "Models\n",
      "(LLMs).\n",
      "Study\n",
      "Key\n",
      "Concepts\n",
      "and\n",
      "Tools:\n",
      "●\n",
      "Understand\n",
      "the\n",
      "key\n",
      "components\n",
      "of\n",
      "an\n",
      "enterprise\n",
      "grade\n",
      "RAG\n",
      "systems\n",
      "○\n",
      "Retrieval-augmented\n",
      "generation\n",
      "(RAG):\n",
      "What\n",
      "it\n",
      "is\n",
      "and\n",
      "why\n",
      "it’s\n",
      "a\n",
      "hot\n",
      "topic\n",
      "for\n",
      "enterprise\n",
      "AI\n",
      "○\n",
      "Advanced\n",
      "RAG\n",
      "for\n",
      "LLMs/SLMs\n",
      "○\n",
      "RAG\n",
      "for\n",
      "Text\n",
      "Generation\n",
      "Processes\n",
      "in\n",
      "Businesses\n",
      "(check\n",
      "part\n",
      "1,\n",
      "3,\n",
      "&\n",
      "4\n",
      "as\n",
      "well)\n",
      "○\n",
      "Langchain\n",
      "Reterivers\n",
      "●\n",
      "Understand\n",
      "the\n",
      "need\n",
      "for\n",
      "advanced\n",
      "prompt\n",
      "engineering\n",
      "in\n",
      "building\n",
      "enterprise\n",
      "grade\n",
      "RAG\n",
      "systems\n",
      "○\n",
      "Full\n",
      "Fine-Tuning,\n",
      "PEFT,\n",
      "Prompt\n",
      "Engineering,\n",
      "and\n",
      "RAG:\n",
      "Which\n",
      "One\n",
      "Is\n",
      "Right\n",
      "for\n",
      "You?\n",
      "○\n",
      "Advanced\n",
      "Prompt\n",
      "Engineering\n",
      "-\n",
      "Practical\n",
      "Examples\n",
      "○\n",
      "Prompt\n",
      "Engineering\n",
      "201:\n",
      "Advanced\n",
      "methods\n",
      "and\n",
      "toolkits\n",
      "○\n",
      "Do\n",
      "you\n",
      "agree\n",
      "with\n",
      "this\n",
      "article?\n",
      "RAG\n",
      "is\n",
      "Just\n",
      "Fancier\n",
      "Prompt\n",
      "Engineering\n",
      "●\n",
      "Understand\n",
      "the\n",
      "need\n",
      "for\n",
      "evaluating\n",
      "RAG\n",
      "components\n",
      "○\n",
      "An\n",
      "Overview\n",
      "on\n",
      "RAG\n",
      "Evaluation\n",
      "○\n",
      "Evaluating\n",
      "RAG:\n",
      "Using\n",
      "LLMs\n",
      "to\n",
      "Automate\n",
      "Benchmarking\n",
      "of\n",
      "Retrieval\n",
      "Augmented\n",
      "Generation\n",
      "Systems\n",
      "○\n",
      "Evaluating\n",
      "RAG\n",
      "Applications\n",
      "with\n",
      "RAGAs\n",
      "○\n",
      "RAG\n",
      "Evaluation\n",
      "Using\n",
      "LangChain\n",
      "and\n",
      "Ragas\n",
      "○\n",
      "RAG\n",
      "System:\n",
      "Metrics\n",
      "and\n",
      "Evaluation\n",
      "Analysis\n",
      "with\n",
      "LlamaIndex\n",
      "○\n",
      "Evaluating\n",
      "RAG\n",
      "Part\n",
      "I:\n",
      "How\n",
      "to\n",
      "Evaluate\n",
      "Document\n",
      "Retrieval\n",
      "○\n",
      "Evaluating\n",
      "RAG/LLMs\n",
      "in\n",
      "highly\n",
      "technical\n",
      "settings\n",
      "using\n",
      "synthetic\n",
      "QA\n",
      "generation\n",
      "○\n",
      "Evaluating\n",
      "Multi-Modal\n",
      "RAG\n",
      "●\n",
      "Understand\n",
      "the\n",
      "tools\n",
      "and\n",
      "techniques\n",
      "to\n",
      "automatically\n",
      "generate\n",
      "RAG\n",
      "evaluation\n",
      "data\n",
      "○\n",
      "The\n",
      "Tech\n",
      "Buﬀet\n",
      "#16:\n",
      "Quickly\n",
      "Evaluate\n",
      "your\n",
      "RAG\n",
      "Without\n",
      "Manually\n",
      "Labeling\n",
      "Test\n",
      "Data\n",
      "○\n",
      "Generating\n",
      "a\n",
      "Synthetic\n",
      "Dataset\n",
      "for\n",
      "RAG\n",
      "○\n",
      "●\n",
      "Learn\n",
      "key\n",
      "packages\n",
      "to\n",
      "planning,\n",
      "building,\n",
      "testing,\n",
      "monitoring,\n",
      "and\n",
      "deploying\n",
      "enterprise\n",
      "grade\n",
      "RAG\n",
      "system\n",
      "○\n",
      "Iterate\n",
      "on\n",
      "LLMs\n",
      "faster:\n",
      "Measure\n",
      "LLM\n",
      "quality\n",
      "and\n",
      "catch\n",
      "regressions\n",
      "○\n",
      "Building\n",
      "RAG-based\n",
      "LLM\n",
      "Applications\n",
      "for\n",
      "Production\n",
      "○\n",
      "ARES:\n",
      "An\n",
      "Automated\n",
      "Evaluation\n",
      "Framework\n",
      "for\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "Systems\n",
      "●\n",
      "Understand\n",
      "the\n",
      "end-to-end\n",
      "technology\n",
      "stack\n",
      "of\n",
      "RAG\n",
      "systems\n",
      "○\n",
      "End-to-End\n",
      "LLMOps\n",
      "Platform○\n",
      "An\n",
      "Enterprise-Grade\n",
      "Reference\n",
      "Architecture\n",
      "for\n",
      "the\n",
      "Production\n",
      "Deployment\n",
      "of\n",
      "LLMs\n",
      "Using\n",
      "the\n",
      "RAG\n",
      "Pattern\n",
      "on\n",
      "Azure\n",
      "OpenAI\n",
      "Task\n",
      "2:\n",
      "Design\n",
      "and\n",
      "Develop\n",
      "the\n",
      "Prompt\n",
      "Generation\n",
      "System\n",
      "●\n",
      "Users\n",
      "can\n",
      "input\n",
      "a\n",
      "description\n",
      "of\n",
      "their\n",
      "objective\n",
      "or\n",
      "task\n",
      "and\n",
      "specify\n",
      "a\n",
      "few\n",
      "scenarios\n",
      "along\n",
      "with\n",
      "their\n",
      "expected\n",
      "outputs.\n",
      "●\n",
      "Write\n",
      "or\n",
      "adopt\n",
      "sophisticated\n",
      "algorithms,\n",
      "you\n",
      "generate\n",
      "multiple\n",
      "prompt\n",
      "options\n",
      "based\n",
      "on\n",
      "the\n",
      "provided\n",
      "information.\n",
      "●\n",
      "This\n",
      "automated\n",
      "prompt\n",
      "generation\n",
      "process\n",
      "saves\n",
      "time\n",
      "and\n",
      "provides\n",
      "a\n",
      "diverse\n",
      "range\n",
      "of\n",
      "alternatives\n",
      "to\n",
      "consider.\n",
      "But\n",
      "add\n",
      "an\n",
      "evaluation\n",
      "metrics\n",
      "that\n",
      "check\n",
      "whether\n",
      "the\n",
      "generated\n",
      "prompt\n",
      "candidate\n",
      "aligns\n",
      "with\n",
      "the\n",
      "input\n",
      "description.\n",
      "Task\n",
      "3:\n",
      "Implement\n",
      "Evaluation\n",
      "Data\n",
      "Generation\n",
      "and\n",
      "Evaluation\n",
      "To\n",
      "further\n",
      "enhance\n",
      "the\n",
      "prompt\n",
      "generation\n",
      "process,\n",
      "incorporate\n",
      "automatic\n",
      "Evaluation\n",
      "Data\n",
      "Generation.\n",
      "●\n",
      "By\n",
      "analysing\n",
      "the\n",
      "description\n",
      "provided\n",
      "by\n",
      "the\n",
      "user,\n",
      "create\n",
      "a\n",
      "set\n",
      "of\n",
      "test\n",
      "cases\n",
      "that\n",
      "serve\n",
      "as\n",
      "evaluation\n",
      "benchmarks\n",
      "for\n",
      "the\n",
      "prompt\n",
      "candidates.\n",
      "●\n",
      "These\n",
      "test\n",
      "cases\n",
      "simulate\n",
      "various\n",
      "scenarios,\n",
      "enabling\n",
      "users\n",
      "to\n",
      "observe\n",
      "how\n",
      "each\n",
      "prompt\n",
      "performs\n",
      "in\n",
      "diﬀerent\n",
      "contexts.\n",
      "●\n",
      "The\n",
      "generated\n",
      "test\n",
      "cases\n",
      "serve\n",
      "as\n",
      "a\n",
      "starting\n",
      "point,\n",
      "sparking\n",
      "creativity\n",
      "and\n",
      "inspiring\n",
      "additional\n",
      "test\n",
      "cases\n",
      "for\n",
      "comprehensive\n",
      "evaluation.\n",
      "Task\n",
      "4\n",
      ":\n",
      "Prompt\n",
      "Testing\n",
      "and\n",
      "Ranking\n",
      "Goals\n",
      "Comprehensive\n",
      "Evaluation:\n",
      "Provide\n",
      "a\n",
      "robust\n",
      "system\n",
      "that\n",
      "uses\n",
      "various\n",
      "methodologies\n",
      "for\n",
      "a\n",
      "thorough\n",
      "assessment\n",
      "of\n",
      "prompts.\n",
      "Customizable\n",
      "and\n",
      "User-Centric:\n",
      "Allow\n",
      "users\n",
      "to\n",
      "choose\n",
      "or\n",
      "customize\n",
      "their\n",
      "preferred\n",
      "evaluation\n",
      "methods.\n",
      "Dynamic\n",
      "and\n",
      "Adaptive:\n",
      "Ensure\n",
      "the\n",
      "system\n",
      "remains\n",
      "ﬂexible\n",
      "and\n",
      "adaptive,\n",
      "capable\n",
      "of\n",
      "incorporating\n",
      "new\n",
      "ranking\n",
      "methodologies\n",
      "as\n",
      "they\n",
      "emerge.\n",
      "Primary\n",
      "Methods\n",
      "●\n",
      "Monte\n",
      "Carlo\n",
      "Matchmaking:\n",
      "This\n",
      "method\n",
      "is\n",
      "used\n",
      "to\n",
      "select\n",
      "and\n",
      "match\n",
      "diﬀerent\n",
      "prompt\n",
      "candidates\n",
      "against\n",
      "each\n",
      "other.\n",
      "The\n",
      "Monte\n",
      "Carlo\n",
      "method,\n",
      "known\n",
      "for\n",
      "its\n",
      "applications\n",
      "in\n",
      "problem-solving\n",
      "and\n",
      "decision-making\n",
      "processes,\n",
      "helps\n",
      "in\n",
      "optimizing\n",
      "the\n",
      "information\n",
      "gained\n",
      "from\n",
      "each\n",
      "prompt\n",
      "battle.\n",
      "By\n",
      "simulating\n",
      "various\n",
      "matchups,\n",
      "it\n",
      "allows\n",
      "the\n",
      "system\n",
      "to\n",
      "test\n",
      "the\n",
      "eﬀectiveness\n",
      "of\n",
      "each\n",
      "prompt\n",
      "in\n",
      "diﬀerent\n",
      "scenarios.\n",
      "●\n",
      "ELO\n",
      "Rating\n",
      "System:\n",
      "This\n",
      "system,\n",
      "which\n",
      "is\n",
      "commonly\n",
      "used\n",
      "in\n",
      "chess\n",
      "and\n",
      "other\n",
      "competitive\n",
      "games,\n",
      "rates\n",
      "the\n",
      "prompts\n",
      "based\n",
      "on\n",
      "their\n",
      "performance\n",
      "in\n",
      "the\n",
      "battles.\n",
      "Each\n",
      "prompt\n",
      "candidate\n",
      "is\n",
      "assigned\n",
      "a\n",
      "rating\n",
      "that\n",
      "reﬂects\n",
      "its\n",
      "success\n",
      "in\n",
      "previous\n",
      "matchups.\n",
      "The\n",
      "system\n",
      "takes\n",
      "into\n",
      "account\n",
      "not\n",
      "just\n",
      "the\n",
      "number\n",
      "of\n",
      "wins\n",
      "but\n",
      "also\n",
      "thestrength\n",
      "of\n",
      "the\n",
      "opponents\n",
      "each\n",
      "prompt\n",
      "has\n",
      "defeated.\n",
      "This\n",
      "rating\n",
      "helps\n",
      "in\n",
      "objectively\n",
      "ranking\n",
      "the\n",
      "prompts\n",
      "based\n",
      "on\n",
      "their\n",
      "eﬀectiveness.\n",
      "Additional\n",
      "Ranking\n",
      "and\n",
      "Matching\n",
      "Mechanisms\n",
      "●\n",
      "TrueSkill\n",
      "Rating\n",
      "System\n",
      ":\n",
      "Ideal\n",
      "for\n",
      "scenarios\n",
      "involving\n",
      "multiple\n",
      "competitors,\n",
      "adjusting\n",
      "ratings\n",
      "based\n",
      "on\n",
      "not\n",
      "just\n",
      "wins\n",
      "and\n",
      "losses\n",
      "but\n",
      "also\n",
      "the\n",
      "uncertainty\n",
      "in\n",
      "performance.\n",
      "●\n",
      "Glicko\n",
      "Rating\n",
      "System:\n",
      "Similar\n",
      "to\n",
      "ELO\n",
      "but\n",
      "with\n",
      "added\n",
      "ﬂexibility,\n",
      "accounting\n",
      "for\n",
      "the\n",
      "volatility\n",
      "in\n",
      "a\n",
      "player's\n",
      "(or\n",
      "prompt’s)\n",
      "performance\n",
      "and\n",
      "the\n",
      "reliability\n",
      "of\n",
      "their\n",
      "rating.\n",
      "●\n",
      "Bayesian\n",
      "Rating\n",
      "Systems\n",
      ":\n",
      "Applies\n",
      "Bayesian\n",
      "inference\n",
      "for\n",
      "a\n",
      "probabilistic\n",
      "approach\n",
      "to\n",
      "rating,\n",
      "considering\n",
      "uncertainties\n",
      "and\n",
      "contextual\n",
      "variations\n",
      "in\n",
      "prompt\n",
      "performance.\n",
      "●\n",
      "Pairwise\n",
      "Comparison\n",
      "Methods\n",
      ":\n",
      "Involves\n",
      "direct\n",
      "comparisons\n",
      "between\n",
      "pairs\n",
      "of\n",
      "prompts,\n",
      "potentially\n",
      "integrating\n",
      "user\n",
      "preferences\n",
      "or\n",
      "expert\n",
      "evaluations\n",
      "into\n",
      "the\n",
      "ranking\n",
      "process.\n",
      "●\n",
      "Categorical\n",
      "Ranking\n",
      ":\n",
      "Instead\n",
      "of\n",
      "a\n",
      "numerical\n",
      "rating,\n",
      "prompts\n",
      "are\n",
      "categorized\n",
      "based\n",
      "on\n",
      "performance\n",
      "criteria\n",
      "like\n",
      "creativity,\n",
      "relevance,\n",
      "etc.,\n",
      "for\n",
      "more\n",
      "qualitative\n",
      "assessments.\n",
      "●\n",
      "Adaptive\n",
      "Ranking\n",
      "Algorithms\n",
      ":\n",
      "Algorithms\n",
      "that\n",
      "learn\n",
      "and\n",
      "adjust\n",
      "over\n",
      "time,\n",
      "considering\n",
      "historical\n",
      "performance\n",
      "data\n",
      "and\n",
      "evolving\n",
      "user\n",
      "preferences\n",
      "or\n",
      "requirements.\n",
      "●\n",
      "Semantic\n",
      "Similarity\n",
      "Matching\n",
      ":\n",
      "Using\n",
      "NLP\n",
      "techniques\n",
      "to\n",
      "match\n",
      "prompts\n",
      "based\n",
      "on\n",
      "semantic\n",
      "content,\n",
      "ideal\n",
      "for\n",
      "understanding\n",
      "nuanced\n",
      "diﬀerences\n",
      "in\n",
      "prompt\n",
      "eﬀectiveness.\n",
      "You\n",
      "should\n",
      "adopt\n",
      "an\n",
      "innovative\n",
      "approach\n",
      "to\n",
      "prompt\n",
      "evaluation\n",
      "by\n",
      "utilizing\n",
      "Monte\n",
      "Carlo\n",
      "matchmaking\n",
      "and\n",
      "ELO\n",
      "rating\n",
      "systems,\n",
      "or\n",
      "any\n",
      "alternative\n",
      "method\n",
      "to\n",
      "match\n",
      "and\n",
      "rank.\n",
      "Task\n",
      "5:\n",
      "User\n",
      "Interface\n",
      "Development\n",
      "Develop\n",
      "a\n",
      "user-friendly\n",
      "interface\n",
      "for\n",
      "interacting\n",
      "with\n",
      "the\n",
      "prompt\n",
      "engineering\n",
      "system.\n",
      "●\n",
      "UI\n",
      "Design:\n",
      "Plan\n",
      "and\n",
      "design\n",
      "a\n",
      "user\n",
      "interface\n",
      "that\n",
      "allows\n",
      "users\n",
      "to\n",
      "easily\n",
      "input\n",
      "data,\n",
      "receive\n",
      "prompts,\n",
      "and\n",
      "view\n",
      "evaluation\n",
      "results.\n",
      "●\n",
      "UI\n",
      "Implementation:\n",
      "Develop\n",
      "and\n",
      "integrate\n",
      "the\n",
      "user\n",
      "interface\n",
      "with\n",
      "the\n",
      "backend\n",
      "prompt\n",
      "engineering\n",
      "system.\n",
      "Task\n",
      "6:\n",
      "System\n",
      "Integration\n",
      "and\n",
      "Testing\n",
      "●\n",
      "Integrate\n",
      "all\n",
      "components\n",
      "of\n",
      "the\n",
      "system\n",
      "and\n",
      "conduct\n",
      "comprehensive\n",
      "testing.\n",
      "●\n",
      "Integrate\n",
      "the\n",
      "prompt\n",
      "generation,\n",
      "Evaluation\n",
      "Data\n",
      "Generation,\n",
      "evaluation,\n",
      "and\n",
      "user\n",
      "interface\n",
      "components.\n",
      "●\n",
      "Test\n",
      "the\n",
      "entire\n",
      "system\n",
      "for\n",
      "functionality,\n",
      "usability,\n",
      "and\n",
      "performance.\n",
      "Reﬁne\n",
      "based\n",
      "on\n",
      "feedback\n",
      "and\n",
      "test\n",
      "results.Tutorials\n",
      "Schedule\n",
      "In\n",
      "the\n",
      "following,\n",
      "the\n",
      "colour\n",
      "purple\n",
      "indicates\n",
      "morning\n",
      "sessions,\n",
      "and\n",
      "blue\n",
      "indicates\n",
      "afternoon\n",
      "sessions.\n",
      "Monday:\n",
      "Understanding\n",
      "Prompt\n",
      "engineering\n",
      "Here\n",
      "the\n",
      "trainees\n",
      "will\n",
      "understand\n",
      "the\n",
      "week’s\n",
      "challenge.\n",
      "●\n",
      "Introduction\n",
      "to\n",
      "Week\n",
      "Challenge\n",
      "(Yabebal)\n",
      "●\n",
      "Introduction\n",
      "and\n",
      "challenge\n",
      "to\n",
      "prompt\n",
      "engineering\n",
      "(Fikerte)\n",
      "Key\n",
      "Performance\n",
      "Indicators:\n",
      "●\n",
      "Understanding\n",
      "week’s\n",
      "challenge\n",
      "●\n",
      "Understanding\n",
      "the\n",
      "prompt\n",
      "engineering\n",
      "●\n",
      "Ability\n",
      "to\n",
      "reuse\n",
      "previous\n",
      "knowledge\n",
      "Tuesday\n",
      "●\n",
      "RAG\n",
      "components\n",
      "(Rehmet)\n",
      "●\n",
      "Techniques\n",
      "to\n",
      "improving\n",
      "R\n",
      "(Retrievers)\n",
      "in\n",
      "RAG\n",
      "(Emitnan)\n",
      "Key\n",
      "Performance\n",
      "Indicators:\n",
      "●\n",
      "Understanding\n",
      "Prompt\n",
      "ranking\n",
      "●\n",
      "Understanding\n",
      "prompt\n",
      "matching\n",
      "●\n",
      "Ability\n",
      "to\n",
      "reuse\n",
      "previous\n",
      "knowledge\n",
      "Wednesday\n",
      "●\n",
      "RAG\n",
      "Evaluation\n",
      "Data\n",
      "Generation\n",
      "(Abel)\n",
      "●\n",
      "Understanding\n",
      "of\n",
      "prompt\n",
      "matching\n",
      "and\n",
      "ranking\n",
      "(Mahlet)\n",
      "Thursday\n",
      "●\n",
      "RAG\n",
      "evaluation\n",
      "metrics\n",
      "(Emitnan)\n",
      "●\n",
      "RAGObs\n",
      "-\n",
      "DevObs\n",
      "of\n",
      "RAG\n",
      "development\n",
      "and\n",
      "production\n",
      "deploymentDeliverables\n",
      "NOTE:\n",
      "Document\n",
      "should\n",
      "be\n",
      "a\n",
      "PDF\n",
      "stored\n",
      "in\n",
      "google\n",
      "drive\n",
      "or\n",
      "published\n",
      "blog\n",
      "link.\n",
      "DO\n",
      "NOT\n",
      "SUBMIT\n",
      "A\n",
      "LINK\n",
      "as\n",
      "PDF!\n",
      "If\n",
      "you\n",
      "want\n",
      "to\n",
      "submit\n",
      "pdf\n",
      "document,\n",
      "it\n",
      "should\n",
      "be\n",
      "the\n",
      "content\n",
      "of\n",
      "your\n",
      "report\n",
      "not\n",
      "a\n",
      "link.\n",
      "Interim\n",
      "Submission\n",
      "-\n",
      "Wednesday\n",
      "8pm\n",
      "UTC\n",
      "●\n",
      "Link\n",
      "to\n",
      "your\n",
      "code\n",
      "in\n",
      "GitHub\n",
      "○\n",
      "Repository\n",
      "where\n",
      "you\n",
      "will\n",
      "be\n",
      "using\n",
      "to\n",
      "complete\n",
      "the\n",
      "tasks\n",
      "in\n",
      "this\n",
      "week's\n",
      "challenge.\n",
      "A\n",
      "minimum\n",
      "requirement\n",
      "is\n",
      "that\n",
      "you\n",
      "have\n",
      "a\n",
      "well\n",
      "structured\n",
      "repository\n",
      "and\n",
      "some\n",
      "coding\n",
      "progress\n",
      "is\n",
      "made.\n",
      "●\n",
      "A\n",
      "review\n",
      "report\n",
      "of\n",
      "your\n",
      "reading\n",
      "and\n",
      "understanding\n",
      "of\n",
      "Task\n",
      "1\n",
      "and\n",
      "any\n",
      "progress\n",
      "you\n",
      "made\n",
      "in\n",
      "other\n",
      "tasks.\n",
      "Feedback\n",
      "You\n",
      "may\n",
      "not\n",
      "receive\n",
      "detailed\n",
      "comments\n",
      "on\n",
      "your\n",
      "interim\n",
      "submission,\n",
      "but\n",
      "will\n",
      "receive\n",
      "a\n",
      "grade.\n",
      "Final\n",
      "Submission\n",
      "-\n",
      "Saturday\n",
      "8pm\n",
      "UTC\n",
      "●\n",
      "Link\n",
      "to\n",
      "your\n",
      "code\n",
      "in\n",
      "GitHub\n",
      "○\n",
      "Complete\n",
      "work\n",
      "for\n",
      "Automatic\n",
      "prompt\n",
      "generation\n",
      "○\n",
      "Complete\n",
      "work\n",
      "for\n",
      "Automatic\n",
      "evaluation\n",
      "○\n",
      "Complete\n",
      "work\n",
      "for\n",
      "Evaluation\n",
      "Data\n",
      "Generation\n",
      "●\n",
      "A\n",
      "blog\n",
      "post\n",
      "entry\n",
      "(which\n",
      "you\n",
      "can\n",
      "submit\n",
      "for\n",
      "example\n",
      "to\n",
      "Medium\n",
      "publishing)\n",
      "or\n",
      "a\n",
      "pdf\n",
      "report.\n",
      ".\n",
      "Feedback\n",
      "You\n",
      "will\n",
      "receive\n",
      "comments/feedback\n",
      "in\n",
      "addition\n",
      "to\n",
      "a\n",
      "grade.References\n",
      "●\n",
      "Meistrari\n",
      "didn’t\n",
      "see\n",
      "a\n",
      "good\n",
      "solution\n",
      "for\n",
      "prompt\n",
      "engineering,\n",
      "so\n",
      "it’s\n",
      "building\n",
      "one\n",
      "●\n",
      "AutoPrompt:\n",
      "Eliciting\n",
      "Knowledge\n",
      "from\n",
      "Language\n",
      "Models\n",
      "with\n",
      "Automatically\n",
      "Generated\n",
      "Prompts\n",
      "●\n",
      "Large\n",
      "Language\n",
      "Models\n",
      "Are\n",
      "Human-Level\n",
      "Prompt\n",
      "Engineers\n",
      "●\n",
      "Prompt\n",
      "Engineering\n",
      "●\n",
      "How\n",
      "to\n",
      "Create\n",
      "a\n",
      "Monte\n",
      "Carlo\n",
      "Simulation\n",
      "using\n",
      "Python\n",
      "●\n",
      "Monte\n",
      "Carlo\n",
      "Method\n",
      "Explained\n",
      "●\n",
      "What\n",
      "is\n",
      "Monte\n",
      "Carlo\n",
      "Simulation?\n",
      "How\n",
      "does\n",
      "it\n",
      "work?\n",
      "●\n",
      "Elo\n",
      "Rating\n",
      "Algorithm\n",
      "●\n",
      "Elo\n",
      "algorithm\n",
      "implementation\n",
      "in\n",
      "Python\n",
      "●\n",
      "TrueSkillTM:\n",
      "A\n",
      "Bayesian\n",
      "skill\n",
      "rating\n",
      "system\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "with open('../data/10 Academy Cohort A - Weekly Challenge_ Week - 6.pdf', 'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the total number of pages in the PDF\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    text = ''\n",
    "\n",
    "    # Loop over each page in the PDF\n",
    "    for page_num in range(num_pages):\n",
    "        # Get the text from the current page\n",
    "        page_text = reader.pages[page_num].extract_text()\n",
    "        # Append the text from the current page to the text string\n",
    "        text += page_text\n",
    "\n",
    "# Now, 'text' contains the text extracted from the PDF file\n",
    "print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2 — at least not without this data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or '3306f52a-a64a-46dd-b81a-0d073fb5a072',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we connect to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\u001b[32m'dimension'\u001b[0m: \u001b[1;36m1536\u001b[0m,\n",
       " \u001b[32m'index_fullness'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       " \u001b[32m'namespaces'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       " \u001b[32m'total_vector_count'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1536\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\u001b[32m'dimension'\u001b[0m: \u001b[1;36m1536\u001b[0m,\n",
       " \u001b[32m'index_fullness'\u001b[0m: \u001b[1;36m0.04838\u001b[0m,\n",
       " \u001b[32m'namespaces'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m''\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'vector_count'\u001b[0m: \u001b[1;36m4838\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       " \u001b[32m'total_vector_count'\u001b[0m: \u001b[1;36m4838\u001b[0m\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/16/24 09:54:13] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-package <a href=\"file://c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\warnings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">warnings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\warnings.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         s\\langchain\\vectorstores\\pinecone.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59</span>: UserWarning: Passing in       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         `embedding` as a Callable is deprecated. Please pass in an Embeddings  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         object instead.                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warnings.warn</span><span style=\"font-weight: bold\">(</span>                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/16/24 09:54:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-package \u001b]8;id=30771;file://c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\warnings.py\u001b\\\u001b[2mwarnings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24658;file://c:\\Users\\ekru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\warnings.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         s\\langchain\\vectorstores\\pinecone.py:\u001b[1;36m59\u001b[0m: UserWarning: Passing in       \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         `embedding` as a Callable is deprecated. Please pass in an Embeddings  \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         object instead.                                                        \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m           \u001b[1;35mwarnings.warn\u001b[0m\u001b[1m(\u001b[0m                                                       \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'http://arxiv.org/pdf/2307.09288'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Llama 2: Open Foundation and Fine-Tuned Chat Models'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'http://arxiv.org/pdf/2307.09288'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Llama 2: Open Foundation and Fine-Tuned Chat Models'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575\u001b[0m\u001b[32m(\u001b[0m\u001b[32m7782\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'http://arxiv.org/pdf/2307.09288'\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m'Llama 2: Open Foundation and Fine-Tuned Chat Models'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
      "\n",
      "    Query: What is so special about Llama 2?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that are optimized for dialogue use cases. These models range in scale from 7 billion to 70 billion parameters, making them quite large and powerful.\n",
      "\n",
      "What sets Llama 2 apart is that its fine-tuned LLMs, specifically the L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models, outperform open-source chat models on most benchmarks tested. Additionally, based on human evaluations for helpfulness and safety, Llama 2 models may serve as suitable substitutes for closed-source models.\n",
      "\n",
      "The Llama 2 models have undergone a detailed approach to fine-tuning and safety, similar to other closed product LLMs like ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, enhancing their usability and safety.\n",
      "\n",
      "The performance of Llama 2 models on helpfulness and safety benchmarks generally surpasses existing open-source models. In fact, they appear to be on par with some closed-source models, at least according to the human evaluations conducted.\n",
      "\n",
      "Overall, the combination of optimal performance, fine-tuning, and safety features makes Llama 2 a special collection of language models for dialogue-based applications.\n",
      "\n",
      "Please note that the information provided is based on the given context.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the provided context, it is mentioned that safety measures were taken in the development of Llama 2. However, the specific details regarding the safety measures used are not mentioned. To obtain more information about the safety measures employed in the development of Llama 2, it would be necessary to refer to the original research paper or publication associated with Llama 2.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the development of Llama 2, several safety measures were employed to enhance the safety of the models. These measures include safety-specific data annotation and tuning, red-teaming, iterative evaluations, and a thorough approach to improving LLM safety.\n",
      "\n",
      "Safety-specific data annotation and tuning involve carefully curating the training data and adjusting the model parameters to prioritize safety and minimize potential harmful outputs.\n",
      "\n",
      "Red-teaming refers to the process of having external experts assess the safety and potential risks associated with the models. This helps identify any vulnerabilities or issues that may have been overlooked during development.\n",
      "\n",
      "Iterative evaluations involve continuously assessing the model's performance and making improvements to enhance its safety. This iterative process helps address any shortcomings or concerns that arise during the evaluation phase.\n",
      "\n",
      "The paper also emphasizes the importance of transparency and openness in sharing the fine-tuning methodology and approach to improving LLM safety. By providing a detailed description of these processes, the aim is to enable the community to reproduce the fine-tuned LLMs and contribute to the ongoing efforts to improve the safety of these models.\n",
      "\n",
      "Overall, these safety measures were implemented to ensure that Llama 2 models are developed responsibly and with a focus on minimizing potential risks and harmful outputs.\n",
      "\n",
      "I hope this answers your question! Let me know if there's anything else I can help with.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much more informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
